1. LeNet-6; VGG-14
2. BatchNorm, LayerNorm, GroupNorm (groups=2 for all layers, including Conv layers); NN = no normalization
3. Initialization Techniques
4. Batch size=100

Ones
LeNet NN: 0.5183
VGG NN: 0.8609

Random Uniform
LeNet BN: 0.1883
LeNet LN: 0.2212
LeNet GN: 0.2171
LeNet NN: 0.2382
VGG BN: 0.1345
VGG LN: 0.1683
VGG GN: 0.1476
VGG NN: 0.6697

Random Normal
LeNet BN: 0.1805
LeNet LN: 0.2054
LeNet GN: 0.1964
LeNet NN: 0.4333
VGG BN: 0.1454
VGG LN: 0.1469
VGG GN: 0.1664
VGG NN: 0.2262

Xavier Uniform
LeNet BN: 0.1061
LeNet LN: 0.1084
LeNet GN: 0.0955
LeNet NN: 0.0938
VGG BN: 0.1034
VGG LN: 0.1027
VGG GN: 0.1061
VGG NN: 0.1050

Xavier Normal
LeNet BN: 0.1007
LeNet LN: 0.1228
LeNet GN: 0.0980
LeNet NN: 0.1002
VGG BN: 0.1031
VGG LN: 0.1244
VGG GN: 0.1095
VGG NN: 0.1192

Kaiming Uniform - mode='fan_in'
LeNet BN: 0.1028
LeNet LN: 0.1088
LeNet GN: 0.1014
LeNet NN: 0.1015
VGG BN: 0.1071
VGG LN: 0.1164
VGG GN: 0.1127
VGG NN: 0.1080

Kaiming Uniform - mode='fan_out'
LeNet BN: 0.1367
LeNet LN: 0.1177
LeNet GN: 0.1078
LeNet NN: 0.1074
VGG BN: 0.1043
VGG LN: 0.1055
VGG GN: 0.1163
VGG NN: 0.1040
